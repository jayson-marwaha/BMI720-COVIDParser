---
title: "Lab 7-8"
author: "Mitchell Izower Jayson Marwaha Lily Payvandi"
date: "10/22/2020"
output: html_document
---

```{r}
#Similar to other file, but here I split X into a test and train set of data, and compare how we did.
library(text2vec)
library(data.table)
library(magrittr)
library(tidyverse)
library(tokenizers)
#install.packages("data.table")
library(data.table)
```

```{r}
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<-as.data.table(X) 
#X$CC comes in as data type = factor, so had to change so tokenization worked
X$CC<-as.character(X$CC) 
X$Cluster<-as.character(X$Cluster)
#changed X$patientID to characters since the identical function later requires this
X$patientID<-as.character(X$patientID)
setDT(X)
setkey(X, patientID)
set.seed(2017L)
all_ids=X$patientID  
train_ids = X$patientID[!is.na(X$Result)]
test_ids = setdiff(all_ids, train_ids)   
#Build the train data set, J is join
train=X[J(train_ids)]  
test = X[J(test_ids)]
# define preprocessing function and tokenization function
#install.packages("tokenizers")
library(tokenizers)
#tolower converts everything into lower case
prep_fun = tolower 
tok_fun = tokenize_lines 
#the various defined terms in here are important, of note: I am using tokenize_lines as the tok_fun since it grabs the whole phrase, I donwloaded the tokenizers package to get more functionality
it_train = itoken(train$CC, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train$patientID, 
             progressbar = FALSE)
vocab = create_vocabulary(it_train)

#Now that we have a vocabulary, we can construct a document-term matrix.
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))

#dtm train is a DTM Document Term Matrix
#check dimensions
dim(dtm_train)  
identical(rownames(dtm_train), train$patientID)  
``` 

```{r}
#install.packages("glmnet")
library(glmnet)
#The train[[']] term is the parallel here to sentiment, either 'Result' or 'Predicted'.  We are training off labeled data, so choosing Result here
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['Result']], 
                              family = 'binomial', 
                              # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))

#This output the AUC curve
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))

```
```{r}
#Testing on testing data,  
# Note that most text2vec functions are pipe friendly!
it_test = tok_fun(prep_fun(test$CC))
# turn off progressbar because it won't look nice in rmd
it_test = itoken(it_test, ids = test$patientID, progressbar = FALSE) 

dtm_test = create_dtm(it_test, vectorizer)

preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$Predicted, preds)
```

```{r}
#install.packages("dlstats")
#install.packages("ROCR")
#install.packages("ROCit") 
library(dlstats)
library(ROCit)
library(ROCR)
#This sets up the test for preds
#This section is pumping out preds for ALL id's
it_test = tok_fun(prep_fun(X$CC))
it_test = itoken(it_test, ids = X$patientID, progressbar = FALSE) 
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1] 

Y<-as.data.frame(preds) 
Y<-rownames_to_column(Y, var="patientID")
Y$patientID<-as.character(Y$patientID)  
Y %>% arrange(patientID)

ZZ<-left_join(Y,X)
ZZZ<-ZZ %>% filter(!is.na(Result))
pred <- prediction(ZZZ$preds, ZZZ$Result)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)

ZZZ %>% select(preds,Result)

```
```{r}
#install.packages("pROC")
library(pROC) 
#plot(roc(ZZZ$Result, ZZZ$preds, percent = TRUE))
roc.ndka <- roc(ZZZ$Result, ZZZ$preds, percent = TRUE)
coords(roc.ndka, "best", ret="threshold", transpose = FALSE, best.method="youden")

```


```{r} 
entire_labeled_dataset <- ZZ %>%
  mutate(Pred_Value = case_when(preds >= 0.03552614	~ 1, preds < 0.03552614	~ 0))

entire_labeled_dataset <- entire_labeled_dataset %>%
  mutate(
    Class2 = case_when(
    Result==0 & Pred_Value==0 ~"TrueNegative",
    Result==1 & Pred_Value==1 ~"TruePositive",
    Result==0 & Pred_Value==1 ~"FalsePositive",
    Result==1 & Pred_Value==0 ~"FalseNegative",
))

FilteredCJC<-entire_labeled_dataset %>% filter(!is.na(Class2)) %>% group_by(Class2,CC) %>% summarise(Count=n()) %>% ungroup()

#Recording most common false positives and false negatives
FalseNeg<-FilteredCJC %>% filter(Class2=="FalseNegative") %>% arrange(desc(Count))
FalsePos<-FilteredCJC %>% filter(Class2=="FalsePositive") %>% arrange(desc(Count))
head(FalseNeg,10)
head(FalsePos,10)

entire_labeled_dataset

#Pick lowest FN, here FN = 48, caveat, estimated J point
entire_labeled_dataset %>% group_by(Class2) %>% summarise(count=n())

write.csv(entire_labeled_dataset, "entire_labeled_dataset.csv")
```